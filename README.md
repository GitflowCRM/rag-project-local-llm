# Lang-Gemma-v2: RAG Backend with Qdrant, NestJS, and Local LLM

## Overview
This project is a Retrieval-Augmented Generation (RAG) backend built with NestJS, PostgreSQL, Qdrant (vector DB), and a local LLM. It allows you to ingest user events, generate embeddings, store them in Qdrant, and answer natural language questions using a combination of semantic search and LLM generation.

**Key Features:**
- ðŸš€ **Smart LLM Model Selection**: Automatically chooses the best model for each query type
- ðŸ“Š **PostHog Events Integration**: Process and analyze user behavior data
- ðŸ§  **LLM-Powered Summaries**: Generate intelligent user behavior summaries
- âš¡ **Queue-Based Processing**: Efficient batch processing with BullMQ
- ðŸŽ¯ **Optimized Context**: Smart metadata extraction for better RAG performance

---

## 1. Feeding Events (Add Data)

### 1.1 Basic Events
Events are stored in PostgreSQL. You can add events via a REST endpoint.

#### Example Event JSON
```json
{
  "user_id": "history",
  "event_type": "fact",
  "event_data": {
    "fact": "The Great Pyramid of Giza was built around 2560 BCE and was the tallest man-made structure for over 3,800 years.",
    "topic": "Ancient Egypt",
    "source": "historical records",
    "category": "architecture"
  },
  "event_timestamp": "2025-06-18T13:22:14.154Z"
}
```

#### Add an Event (Example curl)
```bash
curl --location 'http://localhost:3000/events' \
--header 'Content-Type: application/json' \
--data '{
    "user_id": "history",
    "event_type": "fact",
    "event_data": {"fact": "The Great Pyramid of Giza was built around 2560 BCE and was the tallest man-made structure for over 3,800 years.", "topic": "Ancient Egypt", "source": "historical records", "category": "architecture"},
    "event_timestamp": "2025-06-18T13:22:14.154Z"
}'
```

### 1.2 PostHog Events Processing
For PostHog user behavior data, use the queue-based ingestion system:

#### Trigger PostHog Events Ingestion
```bash
curl --location 'http://localhost:3000/queue/posthog-events/find-users' \
--header 'Content-Type: application/json' \
--data '{
    "batchSize": 50
}'
```

This will:
1. Find users with uningested events
2. Queue individual user processing jobs
3. Generate LLM-powered user summaries
4. Create embeddings and store in Qdrant

---

## 2. Ingest Events to Qdrant (Vector DB)

### 2.1 Basic Events Ingestion
After adding events, you need to ingest them into Qdrant. This will generate embeddings and store them as vectors.

#### Ingest All New Events
```bash
curl --location 'http://localhost:3000/qdrant/ingest' \
--header 'Content-Type: application/json' \
--data '{
    "collection": "events"
}'
```
- Only events not yet ingested (where `ingested_at` is null) will be processed.

### 2.2 PostHog Events Ingestion
PostHog events are automatically processed through the queue system with LLM-generated summaries.

---

## 3. Test Semantic Search

You can search for similar events using a semantic query.

### Example Semantic Search
```bash
curl --location 'http://localhost:3000/qdrant/semantic-search' \
--header 'Content-Type: application/json' \
--data '{
    "collection": "events",
    "query": "What is the oldest pyramid in Egypt?",
    "top": 3
}'
```
- Returns the most relevant events (with payload and similarity score).

---

## 4. Retrieval-Augmented Generation (RAG) - Ask Questions

### 4.1 Basic RAG Queries
You can ask natural language questions and get answers generated by the LLM, grounded in your event data.

#### Example RAG Query
```bash
curl --location 'http://localhost:3000/qdrant/rag-search' \
--header 'Content-Type: application/json' \
--data '{
    "collection": "events",
    "query": "When was the Great Pyramid built and why was it significant?",
    "top": 3
}'
```
- The response includes your query, the generated answer, and the source facts used.

### 4.2 PostHog Events Analysis
Analyze user behavior data with intelligent queries:

#### Example User Behavior Queries
```bash
# Simple counting query (uses fast model)
curl --location 'http://localhost:3000/llm/query' \
--header 'Content-Type: application/json' \
--data '{
    "question": "How many users placed orders in the last 24 hours?",
    "top_k": 10
}'

# Complex analysis query (uses powerful model)
curl --location 'http://localhost:3000/llm/query' \
--header 'Content-Type: application/json' \
--data '{
    "question": "Analyze user behavior patterns and identify trends in shopping behavior",
    "top_k": 15
}'

# Manual model override
curl --location 'http://localhost:3000/llm/query' \
--header 'Content-Type: application/json' \
--data '{
    "question": "What are the most common user activities?",
    "model_override": "microsoft/phi-4-reasoning-plus",
    "top_k": 10
}'
```

---

## 5. Smart LLM Model Selection

The system automatically selects the best model for each query:

### 5.1 Fast Model (`microsoft/phi-4-reasoning-plus`)
Used for:
- Simple queries: "count", "how many", "list", "show"
- Basic summaries and overviews
- User summary generation during ingestion
- Small context windows

### 5.2 Powerful Model (`gemma-3-27b-it`)
Used for:
- Complex analysis: "analyze", "compare", "trend", "pattern"
- Behavioral insights and correlations
- Large context windows (>10K characters)
- Deep reasoning tasks

### 5.3 Manual Override
You can specify any model for specific queries using the `model_override` parameter.

---

## 6. Testing LLM Hallucination (Out-of-Knowledge-Base)

Try asking questions not covered by your data to verify the LLM does not hallucinate:

```bash
curl --location 'http://localhost:3000/qdrant/rag-search' \
--header 'Content-Type: application/json' \
--data '{
    "collection": "events",
    "query": "What will be the next major archaeological discovery in Egypt?",
    "top": 3
}'
```
- The answer should indicate that there is not enough information in the facts to answer the question.

---

## 7. Useful Endpoints

### 7.1 Basic Events
- `POST /events` â€” Add a new event
- `POST /qdrant/ingest` â€” Ingest new events to Qdrant
- `POST /qdrant/semantic-search` â€” Semantic search for similar events
- `POST /qdrant/rag-search` â€” Retrieval-augmented question answering

### 7.2 PostHog Events
- `POST /queue/posthog-events/find-users` â€” Trigger PostHog events ingestion
- `POST /llm/query` â€” Query user behavior data with smart model selection
- `GET /events/posthog/ingested-count` â€” Count ingested PostHog users
- `GET /events/posthog/uningested-count` â€” Count uningested PostHog users

### 7.3 Queue Management
- `GET /queue/status` â€” Check queue status
- `POST /queue/clear` â€” Clear all queues

---

## 8. Running the System

### 8.1 Prerequisites
- Make sure PostgreSQL and Qdrant are running (see `docker-compose.yml`)
- Ensure your local LLM server is running with available models

### 8.2 Start the Application
```bash
# Install dependencies
bun install

# Start the NestJS server
bun start

# Or for development
bun run start:dev
```

### 8.3 Environment Variables
```bash
# LLM Configuration
LLM_API_URL=http://localhost:1234/v1
LLM_MODEL=gemma-3-27b-it
LLM_TEMPERATURE=0.2
LLM_MAX_TOKENS=4096

# Database
DATABASE_URL=postgresql://user:password@localhost:5432/dbname

# Qdrant
QDRANT_URL=http://localhost:6333

# Redis (for queues)
REDIS_HOST=localhost
REDIS_PORT=6379
```

---

## 9. Data Management

### 9.1 Clear Qdrant Collection
To clear all data from a Qdrant collection:

```bash
curl --location 'http://localhost:3000/qdrant/clear-collection' \
--header 'Content-Type: application/json' \
--data '{
    "collection": "events"
}'
```

### 9.2 Reset Ingestion Status
To reset ingestion status and reprocess all events:

```sql
-- Reset basic events
UPDATE events SET ingested_at = NULL;

-- Reset PostHog events  
UPDATE posthog_events SET ingested_at = NULL;
```

---

## 10. Architecture

### 10.1 Components
- **NestJS**: Main application framework
- **PostgreSQL**: Event data storage
- **Qdrant**: Vector database for embeddings
- **BullMQ**: Queue system for batch processing
- **Local LLM**: For text generation and summaries
- **OpenAI/HuggingFace**: For embeddings generation

### 10.2 Data Flow
1. **Events** â†’ PostgreSQL
2. **Queue Processing** â†’ LLM Summary Generation â†’ Embeddings â†’ Qdrant
3. **Query** â†’ Embedding â†’ Semantic Search â†’ LLM Answer Generation

---

## 11. Notes
- Embeddings are generated using OpenAI or HuggingFace (see `EmbeddingsService`)
- LLM answers are generated using your local LLM (see `LlmService`)
- PostHog events are processed with LLM-generated summaries for better semantic understanding
- All endpoints are documented with Swagger (visit `/api` if enabled)
- The system automatically selects the optimal model for each query type

---

For more details, see the code in the `src/` directory or ask for help!



How to clear QDRANT DATA for collection?


curl -X POST \
  "http://localhost:6333/collections/my_collection/points/delete" \
  -H "Content-Type: application/json" \
  -d '{
        "filter": {
          "must": []
        }
      }'